# TRABAJO INDIVIDUAL DE ETL 

El trabajo consiste en procesar varios dataset de diversos formatos, transformar los datos para extraer dato valioso y finalmente generar un archivo de SQL para su posterior carga en cualquier DataLake en un workflow. 

Podes acceder a la consigna completa en el siguiente: [Link](https://github.com/soyHenry/PI01_DATA_ENGINEERING "Link")

## Tecnologías 

Para la elaboración de este workflow se utilizó principalmente **Python** con las siguiente librerías: 
- *Jupyter* - Interfaz 
- *Pandas* - Para manipulación de datos
- *Pathlib* - Para gestión de path 
- *Chardet* - Para identificar el encoding de los archivos 
- *re* - Para gestión de las expresiones regulares
- *mysql.connector *- Para conectar y gestionar base de datos en mySQL

[![Archivos](https://github.com/LilaAlvesDC/workflow_ETL/blob/main/_str/1%20Archivos.JPG "Archivos")](https://github.com/LilaAlvesDC/workflow_ETL/blob/main/_str/1%20Archivos.JPG "Archivos")


# ¿Cómo correr el script en línea usando solo Google Colab? 

1. Para correr el scrip usando google colab podes acceder alguiente [Notebook](https://colab.research.google.com/drive/17YMz4FL8vhD23dS5F3eX6cSC6mUJIgMC?usp=sharing "Notebook") en línea.

2. Luego deberán crear un acceso directo a los datasets usando google drive. Para hacerlo acceder al siguiente [Link](http://drive.google.com/drive/folders/1Rsq-HHomPtQwy7RIWQ574wKcf56LiGq1 "Link") y 

1. Añadir acceso directo a google 
2. Dar acceso a google 
